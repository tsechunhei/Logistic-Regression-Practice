---
title: "Logistic Regression"
output: pdf_document
---

Preliminaries
```{r}
library(magrittr)
library(gridExtra)
library(ggplot2)
library(dplyr)
df = read.csv('wells.csv')
df = df %>%
mutate(log_arsenic = log(arsenic))
```


Histogram of arsenic and larsenic:
```{r}
arsenic_plot = ggplot(df) +
geom_histogram(aes(x = arsenic), binwidth = 0.1, color = 5) +
xlab('micrograms per liter') +
ggtitle('arsenic')

log_arsenic_plot = ggplot(df) +
geom_histogram(aes(x = log_arsenic), binwidth = 0.1, color = 3) +
xlab('log(micrograms per liter)') +
ggtitle('Log arsenic')

grid.arrange(arsenic_plot, log_arsenic_plot, ncol = 2)
```
The arsenic graph is less concentrated than the log graph.



Z-score:
```{r}
df = df %>%
mutate(dist100 = dist / 100,
z_education = (educ - mean(educ)) / sd(educ))
```






Using dist100 to predict switch:
```{r}
fit1 = glm(switch ~ dist100, family = binomial(link = 'logit'), df)
summary(fit1)
```

Plot:
```{r}
ggplot(df, aes(x = dist100, y = switch)) +
geom_jitter(height = 0.15) +
stat_smooth(method='glm',
method.args = list(family = "binomial"),
formula = y ~ x) +
xlab('Distance') +
ylab("Prob")
```



Predicted probability of switching wells for the average household:
```{r}
avg_dist = df %>%
summarize(avg_dist = mean(dist100)) %>%
pull(avg_dist)
predict(fit1, newdata = data.frame(dist100 = avg_dist), type = 'response')
```


Marginal effect of dist100 for the average household:
```{r}
lambda = function(z) {
exp(z) / ((1 + exp(z))^2)
}
linear_predictor1 = predict(fit1, newdata = data.frame(dist100 = avg_dist))
coef(fit1)[-1] * lambda(linear_predictor1)
```


Add columns:
```{r}
df = df %>%
mutate(p1 = predict(fit1, type = 'response'),
pred1 = 1 * (p1 > 0.5))
```

Error rate of the Bayes classifier:
```{r}
df %>%
summarize(error_rate = mean((pred1 == 1 & switch == 0) | (pred1 == 0 & switch == 1)))
```




Using larsenic to predict switch:
```{r}
fit2 = glm(switch ~ log_arsenic, df, family = binomial(link = 'logit'))
fit2
```


Using zeduc to predict switch:
```{r}
fit3 = glm(switch ~ z_education, df, family = binomial(link = 'logit'))
fit3
```


Using dist100, larsenic, and zeduc to pre-dict switch:
```{r}
fit4 = glm(switch ~ dist100 + log_arsenic + z_education, df, family = binomial(link = 'logit'))
fit4
```

Plots:
```{r}
fit2_plot = ggplot(df, aes(x = log_arsenic, y = switch)) +
geom_jitter(height = 0.15) +
stat_smooth(method='glm',
method.args = list(family = "binomial"),
formula = y ~ x) +
xlab('log Arsenic Concentration') +
ylab('Prob')

fit3_plot = ggplot(df, aes(x = z_education, y = switch)) +
geom_jitter(height = 0.15) +
stat_smooth(method='glm',
method.args = list(family = "binomial"),
formula = y ~ x) +
xlab('years of education (z-score)') +
ylab('Prob')
                  
grid.arrange(fit2_plot, fit3_plot, ncol = 2)
```

Marginal effect of each predictor:
```{r}
avg_log_arsenic = df %>%
summarize(avg_log_arsenic = mean(log_arsenic)) %>%
pull(avg_log_arsenic)
mean_household = data.frame(dist100 = avg_dist,
log_arsenic = avg_log_arsenic,
z_education = 0)
linear_predictor4 = predict(fit4, newdata = mean_household)
coef(fit4)[-1] * lambda(linear_predictor4)
```



Error rate of the Bayes classifier:
```{r}
df = df %>%
mutate(p4 = predict(fit4, type = 'response'),
pred4 = 1 * (p4 > 0.5))

df %>%
summarize(error_rate = mean((pred4 == 1 & switch == 0) | (pred4 == 0 & switch == 1)))
```
The error rate of fit4 is  0.3695 and the error rate of fit 1 is 0.4046. The fit 4 performs better.